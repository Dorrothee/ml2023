{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практична робота №4\n",
    "### Студентки групи МІТ-31 (підгрупа 1)\n",
    "### Борук Дарини Ігорівни"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'max_iter': 100}\n",
      "\n",
      "Performance evaluation:\n",
      "* Accuracy: 0.85\n",
      "* Recall: 0.93\n",
      "* F1-Score: 0.81\n",
      "* AUC-ROC: 0.94\n",
      "* Confusion Matrix:\n",
      "[[21  5]\n",
      " [ 1 13]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('classification_dataset.csv')\n",
    "\n",
    "X = data.drop('Target', axis=1)\n",
    "y = data['Target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  #Regularization parameter\n",
    "    'max_iter': [100, 200, 300, 400]  #Maximum number of iterations for the solver\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "#Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "#Fit the GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#Get the best parameters and best model from Grid Search\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "#Make predictions on the test data using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1])\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"\\nPerformance evaluation:\")\n",
    "print(\"* Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"* Recall: {:.2f}\".format(recall))\n",
    "print(\"* F1-Score: {:.2f}\".format(f1))\n",
    "print(\"* AUC-ROC: {:.2f}\".format(roc_auc))\n",
    "print(\"* Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "Performance evaluation:\n",
      "* Accuracy: 0.72\n",
      "* Recall: 0.72\n",
      "* F1-Score: 0.72\n",
      "* Confusion Matrix:\n",
      "[[9 0 2 0]\n",
      " [2 9 0 0]\n",
      " [3 0 6 1]\n",
      " [0 3 0 5]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('multiclass_dataset.csv')\n",
    "\n",
    "X = data.drop('Target', axis=1)\n",
    "y = data['Target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [50, 100, 150]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=13)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"\\nPerformance evaluation:\")\n",
    "print(\"* Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"* Recall: {:.2f}\".format(recall))\n",
    "print(\"* F1-Score: {:.2f}\".format(f1))\n",
    "print(\"* Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3rd task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'estimator__max_depth': 10, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}\n",
      "\n",
      "Performance evaluation:\n",
      "* Accuracy: 0.42\n",
      "* Precision:  [0.67, 0.77, 0.84, 0.59]\n",
      "* Recall:  [0.5, 0.85, 0.76, 0.45]\n",
      "* F1-Score:  [0.57, 0.81, 0.8, 0.51]\n",
      "\n",
      "Confusion matrix for Label 1:\n",
      "[[30  2]\n",
      " [ 4  4]]\n",
      "\n",
      "\n",
      "Confusion matrix for Label 2:\n",
      "[[15  5]\n",
      " [ 3 17]]\n",
      "\n",
      "\n",
      "Confusion matrix for Label 3:\n",
      "[[16  3]\n",
      " [ 5 16]]\n",
      "\n",
      "\n",
      "Confusion matrix for Label 4:\n",
      "[[11  7]\n",
      " [12 10]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('multilabel_dataset.csv')\n",
    "\n",
    "X = data[['Feature1', 'Feature2', 'Feature3']]\n",
    "y = data[['Label1', 'Label2', 'Label3', 'Label4']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'estimator__max_depth': [10, 20, 30],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1, 2, 4],\n",
    "    'estimator__n_estimators': [50, 100, 150]\n",
    "}\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = MultiOutputClassifier(RandomForestClassifier(random_state=13))\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, multilabel_confusion_matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "#The average parameter in precision_score, recall_score, and f1_score functions is set to None to compute scores for EACH label individually\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "conf_matrix = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#Round to hundredths\n",
    "precision = [round(p, 2) for p in precision]\n",
    "recall = [round(r, 2) for r in recall]\n",
    "f1 = [round(f, 2) for f in f1]\n",
    "\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"\\nPerformance evaluation:\")\n",
    "print(\"* Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"* Precision: \", precision)\n",
    "print(\"* Recall: \", recall)\n",
    "print(\"* F1-Score: \", f1)\n",
    "for i, confusion_matrix in enumerate(conf_matrix, start=1):\n",
    "    print(f\"\\nConfusion matrix for Label {i}:\\n{confusion_matrix}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "juppyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
